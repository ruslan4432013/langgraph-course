# Потоковый вывод в LangGraph

Данный учебный проект демонстрирует различные способы реализации потокового вывода (streaming) в LangGraph. Вы узнаете,
как получать обновления состояния, промежуточные результаты и токены LLM в реальном времени.

## Основные примеры

В папке `src/stream_intro/` представлены следующие сценарии:

1. **Стриминг значений (`values`)**: Позволяет получать полное состояние графа после каждого шага. См.
   `update_values.py`.
2. **Стриминг обновлений (`updates`)**: Возвращает только те данные, которые были изменены конкретным узлом на текущем
   шаге. См. `update_stream.py`.
3. **Стриминг токенов**: Демонстрация потокового вывода текста, генерируемого языковой моделью, с использованием
   `astream_events`. См. `stream_tokens.py`.

## Визуализация агента

В проекте реализован агент, который поддерживает контекст беседы и автоматически выполняет суммаризацию диалога при
превышении лимита сообщений.

Ниже представлена схема работы этого агента:

![Визуализация агента](static/img.png)

## Структура проекта

* `src/stream_intro/agent.py` — Определение графа агента с логикой суммаризации.
* `src/stream_intro/llm.py` — Конфигурация языковой модели.
* `src/stream_intro/mini_stream.py` — Простой пример для демонстрации режимов стриминга.
* `src/stream_intro/stream_tokens.py` — Пример работы с `astream_events` для получения токенов.
* `src/stream_intro/update_stream.py` / `update_values.py` — Примеры использования `stream_mode`.

## Как запустить

Для работы примеров необходимо настроить переменные окружения в файле `.env` (используйте `requirements.txt` для
установки зависимостей).
